part1: Theory
part2: Implement the game environment (pygame)
part3: Implement the agent (game env + model)
part4: Implement the model (pytorch)

Part 1 Theory:

Reinforcement learning :- 
RL is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward.
OR
RL is teaching a software agent how to behave in an environment by telling it how good it is doing.


Deep Q learning:
This approach extends reinforcement learning by using a deep neural network to predict the actions.


Notes: 
agent puts everything together:

	Agent 
	- game
	- model
	Training: (loop)
		- state = get_state(game)				//based on the state we calculate the state 
		- action = get_move(state)				//based on the state we calculate the next action 
			- model.predict()
		- reward, game_over, score = game.play_step(action)	//with each action store these values.	
		- remember						//store the new state, old state, game_over state and the score
		- model.train()						//with these then we train our model, he calls it linear_q net (a neural net with a few linear layers)
 


	Game (Pygame)
	-play_step(action)
		-> reward, game_over, score
	
	Model (pytorch)
	Linear_QNet(DQN)				feed forward neural net with a few layers
	- model.predict(state)
		->action
--------------------------------------------------------------------------------------------------------

Reward: 
	- eat food  = 	+10 
	- game over = 	-10 
	- else      = 	 0

Action: 
- only three different types of movements to avoid 180 degree turns - because that would end the game by the snake eating itself
 
	[1,0,0] -> straight			//depends on the current direction 
	[0,1,0] -> right turn			//depends on the current direction 
	[0,0,1] -> left turn			//depends on the current direction 

State:
- it has 11 values 
that is the snake has to know about these values at each point in time. i.e. its environment at every movement point to make the correct choice
[danger straight, danger right, danger left, 

 direction left, direction right, direction up, direction down,

 food left, food right,
 food up, food down ]

all of these are boolean values.
These values represent the normal screen field of view i.e. assume snake is going from the bottom to the top, if the snake is one step away from the top of the screen (field of play)
then danger straight value in the above matrix will be 1.  



diagram:


         	
	                              MODEL
				     			    |  
	      ___________________________________________________
	      |							|						|
STATE  ------> INPUT LAYER ----- HIDDEN LAYER ----- OUTPUT LAYER ------> ACTION 
		  (11)					(3)

we have 11 different states/ boolean values and the hidden layer does the magic and the based on the computation we predict which of the 3 moves we should do next.


Deep Q Learning:
Q value = Quality of Action				//we are trying to improve this value

0. Init Q Value (= init model) 			//initialize model with some random params
Loop:-
1. Choose action (model.predict(state))		//choose the action by calling model.predict(state) or choose a random move sometimes(mostly done in the beginning of the game, since we dont have enough data yet)
2. Perform action 				//
3. Measure reward				//measure the reward and update the q value accordingly and use that to train the model
4. Update Q value (+ train model)

To train the model you have a loss function which you are trying to optimise or minimize with each game attempt.

Bellman Equation:
NewQ(s,a) = Q(s,a) + α[R(s,a) + γmaxQ'(s',a') - Q(s,a)]


Simplified (in pseudocode):
Q = model.predict(state0)
Q_new = R + γ * max(Q(state1))

i.e. new Q value = old Q value + (the discount rate * max value of Q possible at state 1)

Loss Function:
MSE  (mean squared error):
loss = (Q_new - Q)^2

57 mins

