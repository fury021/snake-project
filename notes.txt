part1: Theory
part2: Implement the game environment (pygame)
part3: Implement the agent (game env + model)
part4: Implement the model (pytorch)

Part 1 Theory:

Reinforcement learning :- 
RL is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward.
OR
RL is teaching a software agent how to behave in an environment by telling it how good it is doing.


Deep Q learning:
This approach extends reinforcement learning by using a deep neural network to predict the actions.


Notes: 
agent puts everything together:

	Agent 
	- game
	- model
	Training: (loop)
		- state = get_state(game)				//based on the state we calculate the state 
		- action = get_move(state)				//based on the state we calculate the next action 
			- model.predict()
		- reward, game_over, score = game.play_step(action)	//with each action store these values.	
		- remember						//store the new state, old state, game_over state and the score
		- model.train()						//with these then we train our model, he calls it linear_q net (a neural net with a few linear layers)
 


	Game (Pygame)
	-play_step(action)
		-> reward, game_over, score
	
	Model (pytorch)
	Linear_QNet(DQN)
	- model.predict(state)
		->action
--------------------------------------------------------------------------------------------------------

Reward: 
	- eat food  = 	+10 
	- game over = 	-10 
	- else      = 	 0

Action: 
- only three different types of movements to avoid 180 degree turns - because that would end the gae by the snake eating itself
 
	[1,0,0] -> straight			//depends on the current direction 
	[0,1,0] -> right turn			//depends on the current direction 
	[0,0,1] -> left turn			//depends on the current direction 

State:
- it has 11 values 
that is the snake has to know about these values at each point in time.
[danger straight, danger right, danger left, 

 direction left, direction right, direction up, direction down,

 food left, food right,
 food up, food down ]

all of these are boolean values.



diagram:


         	
	                            MODEL
				      | 
	      ___________________________________________________
	      |							|
STATE  ------> INPUT LAYER ----- HIDDEN LAYER ----- OUTPUT LAYER ------> ACTION 
		  (11)					(3)




Deep Q Learning:
Q value = Quality of Action

0. Init Q Value (= init model) 			//initialize model with some random params
Loop:-
1. Choose action (model.predict(state))		//choose the action by calling model.predict(state) or choose a random move sometimes(mostly done in the beginning of the game)
2. Perform action 				//
3. Measure reward				//measure the reward and update the q value accordingly and use that to train the model
4. Update Q value (+ train model)

To train the model you have a loss function which you are trying to optimise or minimize with each game attempt.





